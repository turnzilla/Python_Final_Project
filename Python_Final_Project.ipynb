{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25a6b8-f063-44f2-8be5-ecb76ff14149",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Base Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d8d9c0-9813-4e13-82a0-ebeb216914f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "terrorist, israel, state, organization, apartheid, earth, vipe, hope, died, clean\n",
      "\n",
      "Topic #2:\n",
      "war, accountable, colonizing, gtgt, hold, guys, need, criminals, hope, died\n",
      "\n",
      "Topic #3:\n",
      "wt, omg, sht, actual, horrific, terrorists, america, countries, creating, destroyed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bturnbull1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Step 1: Scrape tweets\n",
    "query = '#FreePalestine -is:retweet lang:en'\n",
    "tweets = client.search_recent_tweets(query=query, max_results=10)\n",
    "tweet_texts = [tweet.text for tweet in tweets.data]\n",
    "\n",
    "# Step 2: Preprocess\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\S+|#\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "cleaned_tweets = [preprocess(t) for t in tweet_texts]\n",
    "\n",
    "# Step 3: Vectorize and model topics\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(cleaned_tweets)\n",
    "\n",
    "nmf = NMF(n_components=3, random_state=42)\n",
    "nmf.fit(X)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\", \".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f9773-004b-4601-820c-4062b424585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Base Code with Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637976ec-a697-4b5e-81bc-a36f9b2e57a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Importing Packages\n",
    "##First I import the Python regular expressions module which provides pattern-based text cleaning such as removing URLs and hashtags from the tweets.\n",
    "import re\n",
    "##I then import the NLTK (Natural Language Toolkit) and the English stopword list. Stopwords are common words like “the”, “is”, “in” that are removed in the preprocessing so they do not alter the analysis.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "##Then the TF-IDF Vectorizer from scikit-learn is imported to convert text into numbers within a document-term matrix. This tool creates an overall TF-IDF numerical measure which is composed of both Term Frequency (TF) which counts how often \n",
    "##a word appears in a single tweet, and Inverse Document Frequency (IDF) which scores a word based on how rare it is across all the tweets. A high IDF word will appear in fewer tweets than a low IDF word.\n",
    "##Therefore the combined TF-IDF measure highlights words that are frequent in one tweet and rare across others, which is really helpful for finding topic-specific words.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "##I also import Non-negative Matrix Factorization (NMF), which is a topic modeling algorithm that takes the TF-IDF matrix and breaks it down into a set of topics that illustrates the main themes \n",
    "##in a large collection of text. It turns the text into a document-term matrix where each row is a tweet, each column is a word and each number shows how often that word appeared in the tweet.\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "###Step 1: Scrape tweets\n",
    "##Now I move into actually scraping/gathering the tweets from Twitter/X. \n",
    "##First I define the search query, for this test run I only want tweets containing #FreePalestine, excluding retweets, and written in English.\n",
    "query = '#FreePalestine -is:retweet lang:en'\n",
    "##Next I send the query to the Twitter API using Tweepy (I've already authenticated my bearer token required for access)\n",
    "##Given the free-tier Twitter API limits I retrieve only the 10 most recent tweets that match the query to avoiding maxing out my monthly quota too soon.\n",
    "tweets = client.search_recent_tweets(query=query, max_results=10)\n",
    "##Now I extract the text content of each tweet and create a list\n",
    "tweet_texts = [tweet.text for tweet in tweets.data]\n",
    "\n",
    "###Step 2: Preprocess\n",
    "##I have my list of tweet text, now I need to preprocess it before the analysis can begin.\n",
    "##I download the list of English stopwords discussed above in Importing Packages and load the stopwords list as a Pythen set so it can be used efficiently.\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "##I create a preprocess function that removes URLs (http... or www...),Twitter mentions (@username),Hashtags (#hashtag), all non-alphabetic characters, converts all letters to lowercase, \n",
    "##splits text into individual words (tokens), remove stopwords,and reconstructs the cleaned tokens back into a single string that can be used for modeling.\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\S+|#\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "##I then apply this preprocess function to each tweet in the previously created list\n",
    "cleaned_tweets = [preprocess(t) for t in tweet_texts]\n",
    "\n",
    "###Step 3: Vectorize and model topics\n",
    "##The last step is the actual analysis. First I start the TF-IDF vectorizer discussed at the beginning in Importing Packages. This will again remove all English stopwords and will then\n",
    "##rank all the words by their TF-IDF score and only keep the top 1,000 with the highest overall score, the most \"important\" words. \n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "##I then convert the cleaned top 1,000 words into a document-term matrix that the NMF model can use.\n",
    "X = vectorizer.fit_transform(cleaned_tweets)\n",
    "##I start the NMF model by requesting three topics initially to keep the analysis manageable, since using 10 or more topics can become confusing and lead to overlapping themes. \n",
    "##I can adjust the number of topics later once I confirm the model is working well. These three topics are latent themes created de novo from the data—they are not simply selected from \n",
    "##existing words in the matrix. NMF groups terms that frequently co-occur across tweets and identifies overarching topics based on those term groupings.\n",
    "##Setting random_state at 42 makes the model reproducible, this gives the model a starting point so the algorithm starts the same way every time, producing consistent, reproducible results.\n",
    "##Note that any integer would work.\n",
    "nmf = NMF(n_components=3, random_state=42)\n",
    "##The The .fit() function takes the TF-IDF matrix of tweets and runs the NMF algorithm to find the underlying topic structure. After fitting, the model has learned the topic-term relationships\n",
    "##as dicussed above and can now be used to extract or transform data into topics.\n",
    "##Note TF-IDF provides the words and NMF provides the structure, topics as weighted groups of those words\n",
    "nmf.fit(X)\n",
    "##Finally, I want to display the topics so I retrieve the list of 1,000 top words that the TF-IDF originally identified from the text data.\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "##Then I for loop over each topic developed by NMF in the previous step. Each topic is a list of weights for each word.\n",
    "##Note that the NMF output is stored in nmf.components_\n",
    "##This loop prints the topic number using 1-based indexing to make it readable.\n",
    "##Next it prints the top 10 words based on their weights in the NMF topic component, the argsort()[:-11:-1]: sorts indices of the top 10 words by weight, in descending order.\n",
    "##Lastly, I add a blank line between topics for readability.\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\", \".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))\n",
    "    print()\n",
    "\n",
    "###Result\n",
    "##The code produces three topics learned by the NMF model, each represented by a list of the most important words associated with that topic. These words are selected based on their high weights \n",
    "##in the NMF model, which was derived from the TF-IDF vectorized tweet data. I can now conduct a similar analysis of #BlackLivesMatter tweets and use that for further comparison between the \n",
    "##two datasets.\n",
    "    \n",
    "#FP Topic #1:\n",
    "#palestine, wont, forget, use, platforms, humanity, awareness, equality, spread, lets\n",
    "\n",
    "#FP Topic #2:\n",
    "#genocide, revenge, dehumanising, dehumanised, process, gaza, murder, israelis, response, palestinians\n",
    "\n",
    "#FP Topic #3:\n",
    "#fuck, threat, say, morning, louder, hey, death, like, post, war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d12bb92-90e3-40d8-b1de-eb5ad4946f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "###FreePalestine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4475c588-71cd-41ef-be03-62ee0acfd9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP Topic #1:\n",
      "palestine, wont, forget, use, platforms, humanity, awareness, equality, spread, lets\n",
      "\n",
      "FP Topic #2:\n",
      "genocide, revenge, dehumanising, dehumanised, process, gaza, murder, israelis, response, palestinians\n",
      "\n",
      "FP Topic #3:\n",
      "fuck, threat, say, morning, louder, hey, death, like, post, war\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bturnbull1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Step 1: Scrape tweets\n",
    "fp_query = '#FreePalestine -is:retweet lang:en'\n",
    "fp_tweets = client.search_recent_tweets(query=fp_query, max_results=10) \n",
    "fp_tweet_texts = [tweet.text for tweet in fp_tweets.data]  \n",
    "\n",
    "# Step 2: Preprocess\n",
    "nltk.download('stopwords')\n",
    "fp_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_fp(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\S+|#\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = [word for word in text.split() if word not in fp_stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "fp_cleaned_tweets = [preprocess_fp(t) for t in fp_tweet_texts]\n",
    "\n",
    "# Step 3: Vectorize and model topics\n",
    "fp_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "fp_X = fp_vectorizer.fit_transform(fp_cleaned_tweets)\n",
    "\n",
    "fp_nmf = NMF(n_components=3, random_state=42)\n",
    "fp_nmf.fit(fp_X)\n",
    "\n",
    "fp_feature_names = fp_vectorizer.get_feature_names_out()\n",
    "for fp_topic_idx, topic in enumerate(fp_nmf.components_):\n",
    "    print(f\"FP Topic #{fp_topic_idx + 1}:\")\n",
    "    print(\", \".join([fp_feature_names[i] for i in topic.argsort()[:-11:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e1860-069b-480b-bd1e-95bb0af5039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##BLM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1796d09b-412c-4b93-bef5-94de94c21536",
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequests",
     "evalue": "429 Too Many Requests\nToo Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTooManyRequests\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Step 1: Scrape tweets\u001b[39;00m\n\u001b[32m      8\u001b[39m blm_query = \u001b[33m'\u001b[39m\u001b[33m#BlackLivesMatter -is:retweet lang:en\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m blm_tweets = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_recent_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblm_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     10\u001b[39m blm_tweet_texts = [tweet.text \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m blm_tweets.data]  \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Step 2: Preprocess\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tweepy\\client.py:1276\u001b[39m, in \u001b[36mClient.search_recent_tweets\u001b[39m\u001b[34m(self, query, user_auth, **params)\u001b[39m\n\u001b[32m   1184\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"search_recent_tweets( \\\u001b[39;00m\n\u001b[32m   1185\u001b[39m \u001b[33;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[32m   1186\u001b[39m \u001b[33;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1273\u001b[39m \u001b[33;03m.. _Academic Research Project: https://developer.twitter.com/en/docs/projects\u001b[39;00m\n\u001b[32m   1274\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1275\u001b[39m params[\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m] = query\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/2/tweets/search/recent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mend_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpansions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmedia.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnext_token\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplace.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpoll.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msince_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msort_order\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtweet.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muntil_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_auth\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tweepy\\client.py:129\u001b[39m, in \u001b[36mBaseClient._make_request\u001b[39m\u001b[34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_request\u001b[39m(\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m, method, route, params={}, endpoint_parameters=(), json=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    125\u001b[39m     data_type=\u001b[38;5;28;01mNone\u001b[39;00m, user_auth=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    126\u001b[39m ):\n\u001b[32m    127\u001b[39m     request_params = \u001b[38;5;28mself\u001b[39m._process_params(params, endpoint_parameters)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_type \u001b[38;5;129;01mis\u001b[39;00m requests.Response:\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tweepy\\client.py:115\u001b[39m, in \u001b[36mBaseClient.request\u001b[39m\u001b[34m(self, method, route, params, json, user_auth)\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(method, route, params, json, user_auth)\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests(response)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code >= \u001b[32m500\u001b[39m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TwitterServerError(response)\n",
      "\u001b[31mTooManyRequests\u001b[39m: 429 Too Many Requests\nToo Many Requests"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Step 1: Scrape tweets\n",
    "blm_query = '#BlackLivesMatter -is:retweet lang:en'\n",
    "blm_tweets = client.search_recent_tweets(query=blm_query, max_results=10) \n",
    "blm_tweet_texts = [tweet.text for tweet in blm_tweets.data]  \n",
    "\n",
    "# Step 2: Preprocess\n",
    "nltk.download('stopwords')\n",
    "blm_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_blm(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\S+|#\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = [word for word in text.split() if word not in blm_stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "blm_cleaned_tweets = [preprocess_blm(t) for t in blm_tweet_texts]\n",
    "\n",
    "# Step 3: Vectorize and model topics\n",
    "blm_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "blm_X = blm_vectorizer.fit_transform(blm_cleaned_tweets)\n",
    "\n",
    "blm_nmf = NMF(n_components=3, random_state=42)\n",
    "blm_nmf.fit(blm_X)\n",
    "\n",
    "blm_feature_names = blm_vectorizer.get_feature_names_out()\n",
    "for blm_topic_idx, topic in enumerate(blm_nmf.components_):\n",
    "    print(f\"BLM Topic #{blm_topic_idx + 1}:\")\n",
    "    print(\", \".join([blm_feature_names[i] for i in topic.argsort()[:-11:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5034993-5b17-4ccb-8c2f-aba035c520a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Reddit Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede0b56-4525-4122-874f-c1b666dbcf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"YOUR_CLIENT_ID\",\n",
    "    client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "    user_agent=\"your_user_agent\"\n",
    ")\n",
    "\n",
    "subreddit = reddit.subreddit(\"PalestinianRights\")\n",
    "for post in subreddit.new(limit=5):\n",
    "    print(post.title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
